<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SKNet笔记]]></title>
    <url>%2F2020%2F05%2F11%2FSKNet%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[SKNet(Selective Kernel Convolution)是ResNext后时代的产物，它和SENET是一对兄弟，其灵感来源于我们的视觉神经在看物体时会有不同的感受野。SKNet在实际的数据中表现的也很好。 Abstract 在经典的ResNext系列网络中，卷积神经网络的感受野是相同的。但是众所周知，我们人类的视觉系统在观察的时候感受野明显不相同(在观察远处物体时明显感受野会增加)，作者根据这一灵感提出了SKNet----不同分支有不同的感受野，并且增加了对这些分支的Attention操作，让网络自己学习选哪一种感受野。 Introduction 在对猫视觉的研究中，猫的视觉神经在对同一物体的感受野是不同的，这可以使得猫搜集到多尺寸的信息，对于这一特性最经典的应用就是InceptionNets，它通过融合3×3、5×5、7×7三种不同的卷积核来提升网络的效果。 在刚开始对神经视觉的实验中，科学家一度认为猫看木棍时有一个固定的感受野，称为CRF(Classic )。后来很多科学家发现，在猫盯着棍子的时候，如果你在其注意力外放一个物体猫的视觉神经仍有反应，这被称为NCRF(Non-classic)。并且，如果持续的刺激NCRF一段时间，猫的CRF也会发生改变。也就是说猫的视觉神经感受野是有自动调节功能的。可惜的是只有InceptionNets系列利用了这一网络，但是它仍然是固定了三种不同的视野，并且只采用了简单地线性相加，这种线性相加很有可能会削弱视觉上的动态调整特性。 基于上面几点，作者提出了SKNet，该网络拥有自适应感受野。由Split、Fuse、Select三部分组成。 SKNet具有如下优点： 网络参数少 识别效果好 感受野自适应(作者通过增大检测目标的尺寸和缩小背景尺寸来观察感受野的变化) Releated Work 2.1 多分枝卷积神经网络 Highway networks, ResNet, shake-shake, FractalNets, Multilevel ResNets, InceptionNets 2.2 分组卷积、深度可分离卷积、扩张卷积 AlexNet, ResNext Xception, MobileNetV1, MobileNetV2, ShuffleNet 2.3 注意力机制 SENet, BAM, CBAM 2.4 动态卷积 Spatial Transform Networks, Deformable Convolutional Networks Methods 3.1 Split 如上图所示，split主要有两种操作： \(对于输入X\subseteq \mathbb{R}^{H^{`} \times W^` \times C^`}有:\) \(\widetilde{F}:X \to \widetilde{U} \subseteq \mathbb{R}^{H^{`} \times W^` \times C^`}\) \(\widehat{F}:X \to \widehat{U} \subseteq \mathbb{R}^{H^{`} \times W^` \times C^`}\) 其中，两方法都是用高效分组卷积或者深度可分离卷积、BatchNormalization和Relu。如果还想要提高效率，可以把5×5卷积替换成3×3和2×2组成的扩张卷积。 3.2 Fuse 对于不同卷积核的选择实际上还是通过门结构进行控制，为此需要获取到所有的信息，然后让网络决定能否通过。Fuse就是为了获取整个信息而做的融合。 \(U = \widehat{U} + \widetilde{U}\) \(s_c = F_{gp}(U_c)=\frac{1}{H×W}\sum_{i=1}^{H}\sum_{j=1}^{W}U_c(i,j)\) \(z = F_{fc}(s) = Relu(BatchNormalization(Ws)) \quad ,W\in\mathbb{R}^{d×c},z\in\mathbb{R}^{d×1}\) 这里的d表示缩减后的维度，一个典型的取值为32 \(d=max(C/r, L) \quad ,c=32\) 3.3 Select 在该阶段对压缩特征矩阵z使用Soft Attention来让网络动态调整其RF \(a_c = \frac{e^{A_cz}}{e^{A_cz}+e^{B_cz}}\quad b_c = \frac{e^{B_cz}}{e^{A_cz}+e^{B_cz}}\) 其中，\(A、B \in \mathbb{R}^{C×d}\)为扩展矩阵，其目的是为了把d×1的矩阵扩展到C×1。下标c表示矩阵中的第c的元素。 最终的特征图V通过上面计算的Attention权重a,b最终加权获得： \(V_c = a_c\cdot \widetilde{U}_c+b_c\cdot \widehat{U} \quad , a_c+b_c=1\) \(V=[V_1, V_2, \cdots,V_C] \quad V_c \in \mathbb{R}^{H×W}\) 网络结构设计 网络结构图如下： 其中，G表示分组卷积每一组的Kernel size，fc表示SENet中两个全连接层的输出维度，M表示分路个数，r前面已经说明，表示特征的压缩系数。 SENet可以插入到其他的网络结构中( MobileNet, Shuf- fleNet)，这里插入到ResNext中是为了做对比。 实验 5.1 ImageNet 分类任务 5.1.1 （上图中的224、320表示图像增强中的图像裁剪） 5.1.2 与最优模型的对比 (上图中的括号表示相对于ResNext的提升) 5.1.3 轻量级模型对比 5.1.4 参数性能表现 5.2 CIFAR 分类任务 为了衡量网络在轻量数据上的表现，使用CIFAR 100分类和10分类数据来衡量网络的性能 5.3 Ablation Studies 这部分研究主要是为了证明SKNet可以自适应的调整RF，仍然使用ImageNet为基本数据集。 5.3.1 扩张系数D和分组数G D和G的累积会直接影响到RF大小，实验里固定了一条路径的系数，调整另一条路径的系数如下标，其中的Resulted Kernel表示等效的卷积核大小。从图中可以发现，即使感受野等效的情况下，使用小卷积核和扩张系数的搭配要比直接使用大卷积核好得多。 5.3.2 不同大小卷积核的结合 通过实验不难发现，当M(路的个数)增加时，识别的误差越来越小；无论使用多少M，使用SK都比不使用好；当使用SK时，从M=2到M=3提升不大，更推荐使用M=2。 5.4 自适应RF的解释 这一组实验的具体方法是使用同一组数据，但使用不同的目标物体和背景的尺度，进而观察Attention的权重系数来解释自适应的RF。 从实验中不难发现，当目标物体变大的时候，Attention Weight也逐渐变大(5×5的权重系数减去3×3的权重系数)，这表示神经网络及时的将感受野调大，这个结果完全符合预期。 总结 SKNet从神经学中的可调节感受野出发，通过Attention设计了一套自适应RF的网络结构，在不提高网络复杂度的情况下有效地提升了神经网络的正确率，并且拥有充足的可解释性。]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树结构]]></title>
    <url>%2F2019%2F12%2F04%2F%E6%A0%91%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[&gt; 树结构是一个程序员必须十分熟悉的结构，它包括了二叉树以及二叉树的各种变种、多叉树等，将树结构推广便是图，而树的前中后序遍历就对应了图的DFS，树的层序遍历对应了图的广度优先遍历。 除了特殊问题之外，大部分的关于树的问题的根本是在树的遍历。为了加快遍历的速度，产生了多种不同结构的树结构。 其中、二叉排序树、平衡二叉树、B树等都是为了针对数据方便查找而对树中节点做了约束。 线索二叉树设计的目的也很巧妙，充分利用了剩余节点来把树连接成一个类似链表的结构，使得树的遍历更加方便。 满二叉树、完全二叉树是树的节点满足某种规律时对树的一种称呼。 扩充二叉树是为了克服单一遍历顺序无法构建一个完整的二叉树(否则只能通过先序+中序、先序+后序来确定一个二叉树)。]]></content>
      <categories>
        <category>data structure</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neighbor Embedding]]></title>
    <url>%2F2019%2F09%2F10%2FNeighbor-Embedding%2F</url>
    <content type="text"><![CDATA[简单的说，Neighbor Embedding是一种降维方法，而且是无监督的，利用的是各个单词的临近关系 Manifold Learning(流形学习) 对于降维问题的思考在于，我们的降维在高维空间和低维空间看到的东西是不同的： 上图很容易看到，蓝色的点距离绿色的点更近、距离红色的点更远。但是如果我们只是简单的压扁，就会造成蓝色的点距离红色的点更近的错觉，这就是降维导致的信息丢失。 我们流形学习的思想就是：拉平这个高维空间，而不是压扁这个高位空间，以防止丢失信息： Locally Linear Embedding(LLE) 这种算法的思路是保持数据中任意一个元素和其临近的K个元素的关系\(w_{i,j}\)保持不变: \(Find\, a\,set\,of\,w_{i,j}\,minimizing\\\sum_{i}\|x^i\,-\,\sum_{j}w_{i,j}x^j\|_2\) Then find the dimension reduction results \(z^i\) and \(z^j\) based on \(w_{i,j}\): 降维后的新元素\(z^i\)和\(z^j\)之间，也应该保持上述高维空间的关系，即\(w_{i,j}\)保持不变: 这种关系就和《长恨歌》中的一句诗句类似: LLE中，K的选择是一个关键要素：K选的少了高维的关系可能无法维系；K选的多了算法的复杂性很大，效果也不好： Laplacian Eigenmaps]]></content>
      <categories>
        <category>Unsupervised Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2019%2F09%2F04%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.基本数值特征 对于刚拿到的数据，先进行缺失值处理 对于离散值，我们常用 LabelEncoder OneHotEncoder来对其进行编码以方便识别。常用的为pandas中的get_dummies()方法，当然sklearn 也有对应的类 对于二值特征:Sklearn中的二值化类是Binarizer，当然Pandas直接判断也很方便 对于多项式特征:也就是多个特征指标的组合(一般支持向量机用的比较多，乘积、平方等等)，使用PolynomialFeatures来实现。 对于binning(连续)特征:可以做离散化，划分区间分成几个类(比如年龄)，可以直接写map()函数映射。比较常用的方法,可以直接试一试，效果不好说。 分位数切分: 四分位，二分位，四分之三分位等切分成离散的方式，常用pandans的quantile()方法，然后用qcut()的方法进行切分 对数变换(COX-BOX): 对于分布是正态分布假设的算法(可以用偏度来判定)，数据本身可能偏度很大，而做了对数变换有可能使得其更接近对数变换。使用numpy.log()即可。COX-BOX是另一种更复杂的对数变换，其目的也是让数据近似于正态分布。 2.日期特征处理 一般的日期转化为dataTime格式，可以直接把年月日拿出来用的数据(apply)。提取出年月日之后可以当成特征，也可以二次加工。比如四季、早晚、前半年、后半年等等。 整体的思路是:前期多提特征，后期再筛选。 3.文本特征处理 文本特征的难度是如何让计算机认识这些文字，文字本身对于计算机来说很难提取出特征 可以使用nltk工具包进行预处理，具体使用应该专门学习 关于语句的编码，常用的有词袋模型(对输入的词进行编码，句子中出现的词表示为1，未出现的词表示为0，其实就是词汇分布表，并包含有词频，sklearn中有对应的CountVectorizer) 4.时间序列的特征挖掘 统计特征 : 最大值(max)，最小值(min)，均值(mean)，中位数(median)，方差(variance)，标准差(standard variance)，偏度(skewness)，峰度(kurtosis) 关于偏度和峰度，它们是如下两个公式： \[\mu\,=\,\frac{1}{T}\sum_{i=1}^{T}x_i\] \[\sigma^{2}\,=\,\sum_{i=1}^{T}\frac{1}{T}(x_i - \mu)^{2}\] \[skewness(X)\,=\,E[(\frac{X\,-\,\mu}{\sigma})^{3}]\,=\,\frac{1}{T}\,\sum_{i=1}^{T}\frac{(x_i\,-\,\mu)^3}{\sigma^{3}}\] \[kurtosis(X)\,=\,E[(\frac{X\,-\,\mu}{\mu})^4]\,=\,\frac{1}{T}\,\sum_{i=1}^{T}\frac{(x_i\,-\,\mu)^4}{\sigma^4}\] 熵特征 熵是衡量数据确定性和不确定性的指标，在相同的方差、均值和中位数的情况下，entropy越大，系统就越混乱。 entropy公式如下: \(entropy(X)\,=\,-\sum_{i=1}^{\infty}\,P\{x\,=\,x_i\}\,ln(P\{x\,=\,x_i\})\) 上面是信息论中基本的熵公式，下面介绍几个在时间序列中运用的熵: 2.1 Binned Entropy 这种做法是把时间序列进行分桶的操作，计算每个桶内的熵，以此来衡量时间序列的集中程度 如果一个时间序列的 Binned Entropy 较大，说明这一段时间序列的取值是较为均匀的分布在\(\,min(X_T), max(X_T)\,\)之间。如果取值较小，说明其取值是集中在某一段上的. \(binned entropy\,=\,-\sum_{k=0}^{min(maxbin,len(X))}\;p_k\,ln(p_k)\,\cdot\,1_{p_k&gt;0}\) 2.2 Approximate Entropy 这个指标可判断这个时间序列是具备某种趋势还是随机出现。AE这种方法是将一位空间中的时间序列提升到高维空间中，通过高维空间中向量的距离或者相似度来判断一维空间中是否存在某种趋势。算法大致如下: step1: 给定两个参数m,r .其中m表示取多长的子片段分析，r表示投射到高位时两向量是否相近的阈值。需要构造的m维向量如下: \(X_1(m)\,=\,(x_1,\cdots,x_m)\in\mathbb{R},\) \(X_i(m)\,=\,(x_i,\cdots,x_{m+i-1})\in\mathbb{R},\) \(X_{N-m+1}(m)\,=\,(x_{N-m+1},\cdots,x_N)\in\mathbb{R}.\) step2: 通过上述m维向量可以计算出哪些向量和\(X_i(m)\)相似: \(C_i^m(r)\,=\,(number\,of\,X_j(m)\,such\,that\,d(X_i(m),X_j(m)\,\leq\,r)/(N-m+1))\) 在这里，d可以选取\(L^1,L^2,L^P,L^\infty\)范数，这里常用\(L^\infty\)范数. step3: 考虑函数: \(\Phi^m(r)\,=\,(N-m+1)^-1\:.\sum_{i=1}^{N-m+1}ln(C_i^m(r))\) step4: Approximate Entropy可定义为: \(ApEn(m,r)\,=\,\Phi^m(r)-\Phi^{m+1}(r)\) remark: m一般取值2或者3，r&gt;0, r需要根据具体的时间序列进行调整 如果某个时间序列有许多很相似或者重复的时间序列，那么他的Approximate Entropy就相对较小，反之就相对较大。 2.3. Sample Entropy 测试]]></content>
      <categories>
        <category>feature engineering</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word Embedding]]></title>
    <url>%2F2019%2F08%2F30%2FWord-Embedding%2F</url>
    <content type="text"><![CDATA[核心问题：如何用一个vector表示单词？ 1-of-N encoding 这是最简单的做法，相当于对所有的文字做了一个onehot编码，也就是用一个长度为N(N表示单词的个数)的向量来唯一的表示一个单词。 1-of-N encoding 但是这种方法无法表达各个单词之间的关系，比如dog和cat都是动物，解决方法是给他们再做分类 即使这样，这不能表达多个类别之间的关系。比如class1和class2之间其实是有关系的，因为动物是可以做跑跳的，显然这种硬分类也无法完全表达信息。 Word Embedding 把每一个Word都project到一个高维空间中去，这里的高维空间要比N低多得多。 上图中各个点的关系可以通过在高位空间中的距离或者其他的指标来表示，也可以根据不同的标准来分类。 产生这种向量是非监督学习，我们只知道输入不知道输出 可以使用auto-encoder吗？ 显然是不可以的，输入的是onehot编码，其实是学不到什么东西的。 如何通过Word Embedding 来学习上下文信息? 虽然机器不懂得蔡英文和马英九，但是根据上下文是可以把蔡英文和马英九归并成一类的。 Count based（Glove Vector） 如果单词X，Y经常在一块出现，V(X)和V(Y)就会很接近。 具体做法是让V(X)和V(Y)的内积接近于X和Y共同出现的次数\(N_{x,y}\) Predition based 这种方法对单词的表示仍然是onehot。该方法会训练一个NN，输入是一个onehot的单词编码，输出是一个概率向量，表示某个单词紧跟着该单词的可能性大小。 具体说来，其实是将上图中训练好的网络的第一层向量（Z）取出来作为该词汇的特征向量。 为什么这种方法会奏效呢？ 中间的隐藏层，需要把同类或者相近的词汇投射到相同的区间。只有这样才能降低最终的loss。这种方法当然自动的考虑了上下文关系。 只用一个单词来预测下一个单词肯定是不太现实， 所以引入了Sharing Parameters的模型 其实就是用前N个词汇来预测下一个词汇 如上图，这里前N个单词的权重是共享的，对应的连接处是相同的。其原因一是为了计算方便，二是为了保证同一个单词在不同位置输入得到的特征向量是相同的(比如，就职前面的蔡英文和马英九不应该因为顺序不同而得到大相径庭的结果)。 上面的公式给了一个等价变换。要得到单词的embedding，在训练完之后，只需要乘以那个W即可得到对应的embeddding。 另外在实际训练的时候，为了保持W相同，应该做到： 这个想法十分的巧妙，对反向传播做了小小的改动。 基本的训练过程 当然变形的训练有好多种，他们的优势都各有千秋： Continues bug of word 之前是考虑上文，这里改成了考虑上下文 Skip-gram 用中间的Word来预测上下文 应用 对于我们训练出来的词向量，还有很多有意思的操作。 比如对两个词向量做减法，就能得到一些规律: \(V(hotter)-V(hot)\,\approx\,V(bigger)-V(big)\) \(V(Rome)-V(Italy)\,\approx\,V(Berlin)-V(Germany)\) \(V(king)-V(queen)\,\approx\,V(uncle)-V(aunt)\) 那么我们的机器就可以推测，罗马的意大利就和柏林的？？一样？ \(Compute\,V(Berlin)-V(Rome)+V(Italy)\) 之后就寻找最接近上述结果的词向量，就能找到答案 我们还可以做多语言的Word Embedding，中英文词汇翻译也可以做到 就像上面一样，我们可以学习一些对应关系，然后就可以进行翻译这种功能了。 除了对文字的Embedding，还可以对图像做 这种方式可以用来做一些分类功能，因为传统的方法无法区分新增加的类别。这种方法即使没有这一类，至少也能区别出来不是已知的类别 我们甚至可以多document做Embedding，最简单的方法就是对文档做词袋，然后用auto-encoder。但是词袋无法考虑语言的顺序，会失去很多的信息。 下面是一些解决办法，需要深入研究可以直接去拜读。]]></content>
      <categories>
        <category>Unsupervised Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记]]></title>
    <url>%2F2019%2F08%2F30%2F%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[闲暇之余，想写篇日记，记录自己的所见所闻. 我的博客已经三个月没有更新东西了， 这个三个月可能发生了一些事情，也可能没有发生什么事情，但是终究还是发生了一些事情的。 &quot;我太难了&quot; &quot;我太难了&quot;， 当收到导师召回的消息时，这句话便脱口而出。纵观我没几天的暑假，这可能是最悲惨的消息了。我的暑假分布大概是这样的：98%的时间去练习科目三并最后准备考驾照，2%的时间去跪棚。整个暑假都在奔波中度过。 那天是台风利奇马来临的日子，刚开始它还是听和蔼的，只是淅淅沥沥的小雨，甚至给人一种 渭城朝雨浥轻尘，客舍青青柳色新 的错觉。我踏着单车，带着一种刚回家的喜悦奔向了科目三的练习地址。 “这不还在市区么，虽然是外环，有地图APP肯定没问题”， 自信满满的我如是道。 开始只是濛濛细雨，周围是一条小河，这种环境让我似乎穿梭在江南的小镇。我顺着怡人的柏油路，一边骑车一边哼着小曲赶往目的地。 “这地图为啥没标出来？”我一阵惊呼，这是到目的地的必经之路--一座桥。不幸的是这座桥似乎正在被修，因为它被封死了。 雨兄似乎也很给力，知道我认不得路的时候顺便增大了雨势。嗯，真的是加量不加价呢。 经过一番挣扎和一顿问路后，我终于曲曲折折的绕到了练车的地方。此种心酸不必多说，比如全身湿透这种基本操作，可以尽情脑补这种囧事。 总而言之，我还是到了，虽然晚了半个小时。 “XXX教练今天休息，你明天再来吧”。 当时我的心里其实是很平静的。“真正的勇士，敢于正视淋漓的大到暴雨，敢于直面被人放鸽子的惨淡人生”，这可能是我当初最好的感受。此时的雨兄可以称得上是大到暴雨了，我的心里也是大到暴雨。 未完待遇]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Set Matrix Zeroes]]></title>
    <url>%2F2019%2F05%2F27%2FSet-Matrix-Zeroes%2F</url>
    <content type="text"><![CDATA[问题描述 给矩阵置0，就是找矩阵中为0的元素，将其同行同列置0,必须是就地算法（in-place） 1. Example 1: - input: [ [1,1,1], [1,0,1], [1,1,1] ] - output: [ [1,0,1], [0,0,0], [1,0,1] ] Example 2: input: [ [0,1,2,0], [3,4,5,2], [1,3,1,5] ] output: [ [0,0,0,0], [0,4,5,0], [0,3,1,0] ] 整体思路 思路1.0版本，朴素想法 最朴素想法，先找为0的坐标，然后将其同行同列置0。这里有个小trick，找到为0的地方不能立即置0，下侧和右侧会导致判断失误。 代码1.0 123456789101112131415161718192021222324252627282930 private void setZeroes(int[][] matrix) &#123; //最朴素想法，先找，然后置0 //注意：置0不能影响后面 HashSet&lt;Integer&gt; row = new HashSet&lt;&gt;(), col = new HashSet&lt;&gt;(); if (matrix.length==0) return; int m = matrix.length, n = matrix[0].length; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (matrix[i][j]==0)&#123;// 填充0 row.add(i); col.add(j); &#125; &#125; &#125;// 置0 for (int i : row) &#123; for (int k = 0; k &lt; n; k++) matrix[i][k] = 0; &#125; for (int j : col) &#123; for (int k = 0; k &lt; m; k++)&#123; matrix[k][j] = 0; &#125; &#125; for (int[] nums : matrix) &#123; System.out.println(Arrays.toString(nums)); &#125; &#125; 思路2.0版本 上个版本肯定是不work的啦，毕竟最朴素想法时空只能打败30%左右的样子。作为一个never setter的人，怎么能容忍这么高的时空复杂度。 上一种方法空间复杂度为O(m*n),我想办法降到O(1)。注意到当检查到matrix[i][j] == 0 ,不能直接所有行 列置0的原因是会影响下侧和右侧的判断。但是上侧和左侧不会影响，故我们不再使用HashMap，直接将matrix[i][0] = matrix[0][j] = 0 ,然后再检查一下行列开头即可。注意如果本来第0行或者第0列就有0，需要用一个flag来记忆一下，然后再判定置0。 代码2.0 123456789101112131415161718192021222324252627282930313233343536373839404142private void setZeroes2(int[][] matrix)&#123; int m = matrix.length, n = matrix[0].length; boolean isCol = false, isRow = false; for (int k = 0; k&lt;m; ++k)&#123; if (matrix[k][0] == 0) &#123; isCol = true; // 第一列应当置0 break; &#125; &#125; for (int k = 0; k&lt;n; ++k)&#123; if (matrix[0][k] == 0)&#123; isRow = true; // 第一行应当置0 break; &#125; &#125; for (int i = 1; i &lt; m; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; if (matrix[i][j]==0)&#123;// 填充0 matrix[i][0] = 0; matrix[0][j] = 0; &#125; &#125; &#125; for (int i = 1; i&lt;m; i++)&#123; if (matrix[i][0] == 0)&#123; for (int j = 1; j&lt;n; ++j) matrix[i][j] = 0; &#125; &#125; for (int j = 1; j&lt;n; j++)&#123; if (matrix[0][j] == 0)&#123; for (int i = 1; i&lt;m; ++i) matrix[i][j] = 0; &#125; &#125; if (isRow) for (int k = 0; k&lt;n; ++k) matrix[0][k] = 0; if (isCol) for (int k = 0; k&lt;m; ++k) matrix[k][0] = 0; for (int[] nums : matrix) &#123; System.out.println(Arrays.toString(nums)); &#125; &#125; 中等题就是这样，解出来比较简单，但是想要拿个top还是比较难的。不管怎样，第二种解法也是top 98%的存在。那么就来个九转大肠鼓励一下自己吧！]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Regular Expression Matching]]></title>
    <url>%2F2019%2F05%2F26%2FRegular-Expression-Matching%2F</url>
    <content type="text"><![CDATA[问题描述 字符匹配问题 用p来匹配s. p中可能包含. * . 表示匹配任一个单一字符 * 表示匹配0到多个前一个字符 要求p和s匹配 Example 1: Input: &gt; s=&quot;aa&quot; &gt; p = &quot;a&quot; Output: false Explanation: &gt; &quot;a&quot; does not match the entire string &quot;aa&quot; Example 2: Input: &gt; s=&quot;aa&quot; &gt; p = &quot;a*&quot; Output: true Explanation: &gt; &quot;'*' means zero or more of the precedeng element, 'a'. Therefore, by repeating 'a' once, it becomes &quot;aa&quot; Example 3: Input: &gt; s=&quot;aab&quot; &gt; p = &quot;c*a*b&quot; Output: true Explanation: &gt; c can be repeated 0 times, a can be repeated 1 time. Therefore it matches &quot;aab&quot;. 思路 这是一道非常困难的题, 曾经害我苦思冥想了好几天。我刚开始甚至没想到居然能用DP解这个问题，后来苦苦求索翻阅leetcode一众大神的解释后才搞明白这个题用DP到底怎么搞 这是一道true false dp问题, dp[i][j] 表示了s前i个元素和p前j个元素的匹配情况 分情况讨论： 如果s[i] == p [j] || p[j] == '.' 这时候完全就看前面的情况 dp[i][j] = dp[i-1][j-1] 如果s[i] != p [j]这个也要分情况 2.1 如果p[j] == '*' (ba a* ab a*) 2.1.1 若 p[j-1] != s[i] &amp;&amp; p[j-1] != '.' 此时, 由于上一个元素不匹配导致*无法复制，那么*只能让上一个元素清空: &gt; dp[i][j] = dp[i][j-2] 2.1.2 除上面的情况外，即可以复制上一个元素达到匹配的目的，当然也可以不复制上一个元素: &gt; dp[i][j] = dp[i][j-2] (上一个元素清空) || dp[i][j-1] (*只代表一个元素) || dp[i-1][j] (代表多个元素，如果在i前面都能和p匹配，那加一个自然也能匹配*) 2.2 如果p[j] != '*' &gt; dp[i][j] = false 特别解释一下，为什么在2.1.2中，当代表多个元素时，匹配的是dp[i-1][j]这个奇怪的搭配。让我们来举个栗子: 假设我们的有 &gt; s: abbbbb &gt; p: cb* 我们看到这里的*，实际上是代表了5个b,但是当我们求dp[i][j]的时候，我们无法得匹配完这些b之后前面的元素是否匹配，我们删掉这些b &gt; s: a &gt; p: c 也就是说，a,c是否匹配已经在之前迭代了。如何得知a,c的迭代位置呢？ &gt; dp[i][j] = dp[i-1][j] = …… = dp[i-6][j] 这是通过我们之前已经求到的结果迭代出来的，你会发现i递减的过程其实就是在找重复元素之前的元素，所以我们直接给出了dp[i-1][j] 标准代码 其实如果能看懂上面的解释的话，代码不成问题，上面的解释已经接近于伪代码了。 12345678910111213141516171819202122public boolean isMatch(String s, String p) &#123; boolean[][] dp = new boolean[s.length()+1][p.length()+1]; // s为空, p为空 dp[0][0] = true; // s为空， p不空 for (int j = 1; j &lt;= p.length(); j++) &#123; if (p.charAt(j-1)=='*' &amp;&amp; dp[0][j-2]) dp[0][j] = true; &#125; //s不空，p空，直接默认false for (int i = 1; i &lt; s.length() + 1; i++) &#123; for (int j = 1; j &lt; p.length() + 1; j++) &#123; if (s.charAt(i-1)==p.charAt(j-1) || p.charAt(j-1)=='.') dp[i][j] = dp[i-1][j-1]; else &#123; if (p.charAt(j-1)=='*')&#123; if (p.charAt(j-2)!=s.charAt(i-1)&amp;&amp;p.charAt(j-2)!='.') dp[i][j] = dp[i][j-2]; else dp[i][j] = dp[i][j-2] || dp[i][j-1] || dp[i-1][j]; &#125; &#125; &#125; &#125; return dp[s.length()][p.length()]; &#125; 终于啃完这个头疼的问题了，看张图片奖励下自己吧]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClimbingStairs]]></title>
    <url>%2F2019%2F05%2F25%2FClimbingStairs%2F</url>
    <content type="text"><![CDATA[问题描述 这道题是easy题： You are climbing a stair case. It takes n steps to reach to the top. Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top? 1. example1: Input: 2 Output: 2 Explanation: 1. 1 step + 1 step 2. 2 steps 2. example2: Input: 3 Output: 3 Explanation: 1. 1 step + 1 step + 1 step 2. 1 step + 2 steps 3. 2 steps + 1 step 思路 这个就很简单了, 典型的计数型动态规划问题，常规做法： 记dp[n] 表示到高度n有dp[n]种方法，则dp[n] = dp[n-1]+dp[n-2] 边界： dp[0] = 1 dp[1] = 2 计算顺序：从左到右 代码 1234567891011public int climbStairs(int n) &#123; int[] dp = new int[n]; dp[0] = 1; if (n&gt;1) dp[1] = 2; if (n&gt;2)&#123; for (int i = 2; i &lt; n; i++) &#123; dp[i] = dp[i-1]+dp[i-2]; &#125; &#125; return dp[n-1]; &#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[孔雀东南飞（中文测试）]]></title>
    <url>%2F2019%2F05%2F24%2F%E4%B8%AD%E6%96%87%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[孔雀东南飞插图 汉末建安中，庐江府小吏焦仲卿妻刘氏，为仲卿母所遣，自誓不嫁。其家逼之，乃投水而死。仲卿闻之，亦自缢于庭树。时人伤之，为诗云尔。 孔雀东南飞，五里一徘徊。 十三能织素6，十四学裁衣，十五弹箜篌7，十六诵诗书8。十七为君妇，心中常苦悲。君既为府吏，守节9情不移。贱妾留空房，相见常日稀。鸡鸣入机织，夜夜不得息。三日断10五匹，大人故嫌迟11。非为织作迟，君家妇难为！妾不堪12驱使，徒13留无所施14。便可白公姥15，及时相遣归 府吏得闻之，堂上启阿母：“儿已薄禄相16，幸复得此妇，结发17同枕席，黄泉共为友。共事二三年，始尔18未为久。女行无偏斜，何意致不厚19？” 阿母谓府吏：“何乃太区区20！此妇无礼节，举动自专由21。吾意久怀忿，汝岂得自由！东家有贤22女，自名秦罗敷，可怜23体无比，阿母为汝求。便可速遣之，遣去慎莫留！” 府吏长跪告：“伏惟24启阿母，今若遣此妇，终老不复取25！” 阿母得闻之，槌床26便大怒：“小子无所畏，何敢助妇语！吾已失恩义，会不相从许27！” 府吏默无声，再拜还入户。举言28谓新妇29，哽咽不能语：“我自不驱卿，逼迫有阿母。卿但暂还家，吾今且报府30。不久当归还，还必相迎取。以此下心意31，慎勿违吾语。” 新妇谓府吏：“勿复重纷纭32。往昔初阳岁33，谢34家来贵门。奉事循公姥，进止敢自专？昼夜勤作息35，伶俜萦苦辛36。谓言37无罪过，供养卒38大恩；仍更被驱遣，何言复来还！妾有绣腰襦39，葳蕤40自生光；红罗复斗帐，四角垂香囊；箱帘41六七十，绿碧青丝绳，物物各自异，种种在其中。人贱物亦鄙，不足迎后人42，留待作遗施43，于今无会因44。时时为安慰，久久莫相忘！” 鸡鸣外欲曙，新妇起严妆45。著我绣夹裙，事事四五通46。足下蹑47丝履，头上玳（dài）瑁（mào）48光。腰若流纨素，耳著明月珰49。指如削葱根，口如含朱丹。纤纤作细步，精妙世无双。 上堂拜阿母，阿母怒不止。“昔作女儿时50，生小出野里51。本自无教训，兼愧52贵家子。受母钱帛多，不堪母驱使。今日还家去，念母劳家里。”却53与小姑别，泪落连珠子。“新妇初来时，小姑始扶床；今日被驱遣，小姑如我长。勤心养公姥，好自相扶将54。初七及下九55，嬉戏莫相忘。”出门登车去，涕落百余行。 府吏马在前，新妇车在后。隐隐56何甸甸，俱会大道口。下马入车中，低头共耳语：“誓不相隔卿，且暂还家去；吾今且赴府，不久当还归。誓天不相负！”57 新妇谓府吏：“感君区区58怀！君既若见录59，不久望君来。君当作磐石，妾当作蒲苇，蒲苇纫60如丝，磐石无转移。我有亲父兄61，性行暴如雷，恐不任我意，逆62以煎我怀。”举手长劳劳63，二情同依依。 入门上家堂，进退无颜仪64。阿母大拊掌65，不图子自归66：“十三教汝织，十四能裁衣，十五弹箜篌，十六知礼仪，十七遣汝嫁，谓言无誓违67。汝今何罪过，不迎而自归？”兰芝惭阿母：“儿实无罪过。”阿母大悲摧68。 还家十余日，县令遣媒来。云有第三郎，窈窕69世无双。年始十八九，便言多令才70。 阿母谓阿女：“汝可去应之。” 阿女含泪答：“兰芝初还时，府吏见丁宁71，结誓不别离。今日违情义，恐此事非奇72。自可断来信73，徐徐更谓之74。” 阿母白媒人：“贫贱有此女，始适75还家门。不堪76吏人妇，岂合令郎君？幸可广问讯，不得便相许。” 媒人去数日，寻遣丞请还，说有兰家女，丞籍有宦官77。云有第五郎，娇逸78未有婚。遣丞为媒人，主簿79通语言。直说太守家，有此令郎君，既欲结大义，故遣来贵门。 阿母谢媒人：“女子先有誓，老姥岂敢言！” 阿兄得闻之，怅然心中烦。举言谓阿妹：“作计80何不量81！先嫁得府吏，后嫁得郎君，否泰82如天地，足以荣汝身。不嫁义郎83体，其往欲何云84？” 兰芝仰头答：“理实如兄言。谢家事夫婿，中道还兄门。处分85适86兄意，那得自任专！虽与府吏要87，渠会88永无缘。登即89相许和，便可作婚姻。“ 媒人下床去，诺诺复尔尔90。还部白府君91：“下官92奉使命，言谈大有缘93。”府君得闻之，心中大欢喜。视历94复开书，便利此月内，六合95正相应。良吉三十日，今已二十七，卿96可去成婚。交语97速装束，络绎如浮云。青雀白鹄舫98，四角龙子幡99。婀娜100随风转，金车玉作轮。踯躅101青骢马102，流苏103金镂鞍。赍104钱三百万，皆用青丝穿。杂彩105三百匹，交广106市鲑107珍。从人四五百，郁郁108登郡门。 阿母谓阿女：“适109得府君书，明日来迎汝。何不作衣裳？莫令事不举110！” 阿女默无声，手巾掩口啼，泪落便如泻。移我琉璃榻111，出置前窗下。左手持刀尺，右手执绫罗。朝成绣夹裙，晚成单罗衫。晻晻112日欲暝，愁思出门啼。 府吏闻此变，因求假暂归。未至二三里，摧藏113马悲哀。新妇识马声，蹑履相逢迎。怅然遥相望，知是故人来。举手拍马鞍，嗟叹使心伤：“自君别我后，人事不可量114。果不如先愿，又非君所详。我有亲父母115，逼迫兼弟兄116。以我应他人，君还何所望！” 府吏谓新妇：“贺卿得高迁！磐石方且厚，可以卒千年；蒲苇一时纫，便作旦夕间。卿当日胜贵117，吾独向黄泉！” 新妇谓府吏：“何意出此言！同是被逼迫，君尔妾亦然。黄泉下相见，勿违今日言！”执手分道去，各各还家门。生人作死别，恨恨118那可论？念与世间辞，千万不复全！ 府吏还家去，上堂拜阿母：“今日大风寒，寒风摧树木，严霜结庭兰。儿今日冥冥119，令母在后单120。故121作不良计122，勿复怨鬼神！命如南山石，四体123康且直124！” 阿母得闻之，零泪应声落：“汝是大家子，仕宦于台阁125。慎勿为妇死，贵贱情何薄126！东家有贤女，窈窕艳城郭，阿母为汝求，便复在旦夕。” 府吏再拜还，长叹空房中，作计乃尔立127。转头向户里，渐见愁煎迫。 其日牛马嘶，新妇入青庐128。奄奄129黄昏130后，寂寂人定初。“我命绝今日，魂去尸长留！”揽裙脱丝履，举身赴清池。 府吏闻此事，心知长别离。徘徊庭树下，自挂东南枝。 两家求合葬，合葬华山131傍。东西植松柏，左右种梧桐。枝枝相覆盖，叶叶相交通132。中有双飞鸟，自名为鸳鸯。仰头相向鸣，夜夜达五更。行人驻足133听，寡妇起彷徨。多谢134后世人，戒之慎勿忘！ [10]]]></content>
      <categories>
        <category>文言文</category>
      </categories>
      <tags>
        <tag>文学</tag>
      </tags>
  </entry>
</search>
